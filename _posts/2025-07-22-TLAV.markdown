---
layout: post
title:  Thinking Like a Vertex
date:   2025-07-22 15:30:00 +0300
tags:   ParallelProgramming
description:  分布式大型图处理
---

# [简介](#简介)

文章全名：
Thinking Like a Vertex: a Survey of Vertex-Centric Frameworks for Large-Scale Distributed Graph Processing.  
1. 大型图处理框架；   
2. 迭代式图算法；   
3. 并行化抽象思想；   
4. 顶点中心化的并行方式；   
   
## [MapReduce](MapReduce)

参考文献：MapReduce: Simplified Data Processing on Large Clusters.    
1. 大数据现状：2014年3月，IBM报告当今世界90%的数据是在近两年生产的，大数据处理的问题已经摆在了众人面前；    
2. 趋势：分析系统开始从“共享的”、“中心化的”处理框架发展向“分布式的”、“去中心化的”的处理框架；     
3. MapReduce框架展示了一种程序范式去充分发挥分布式算法的能力，而隐藏掉底层级的细节。MapReduce把任务分为Map阶段和Reduce阶段。Map函数处理一个键值对，生成一组中间级的键值对，Reduce函数合并所有具有相同键的中间级的值。     
   
![]({{ site.baseurl }}/images/TLAV-0001.jpg)  

4. 面对图或矩阵数据表达的多次迭代的算法时，效率低下；
   
## [Bulk Synchronous Parallel](Bulk-Synchronous-Parallel)

参考文献：
A Bridging Model for Parallel Computation.
1. 冯诺依曼模型是软件与硬件之间的桥梁，批量同步并行（BSP）在并行计算上也扮演了桥梁的角色；     
   
![]({{ site.baseurl }}/images/TLAV-0002.jpg)      

2. 相对于分布式共享内存，BSP模型充分利用了消息通讯协议(MPI)，其能避免高延迟读取，死锁，竞争条件。BSP是两步式执行过程，能进行迭代和同步。     
3. BSP的两步式执行方式如下：    
1）对本地数据执行计算任务；   
2）对计算结果进行通信，并重复这两步；   
4. 一次计算和通信组成一个超级步    
   
## [Graph Parallel Systems]([Graph-Parallel-Systems)

1. Pregel系统：其用BSP模型为图算法定制了API，使其可以让工程师表达”think like a vertex“(TLAV).       
1）一个顶点执行用户定义的顶点函数，并把结果发送到相邻节点；    
2）在同步栅栏处终止超级步，保证在当前超级步发生信息，在下一个超级步接收信息；   
3）顶点根据执行的状态改变其自身状态，自身拥有两个状态”激活态“和”非激活态“；    
4）pergel终止时，所有顶点都结束任务且不再有顶点间的信息交换；  

2. BSP和TLAV:
1) BSP是一个一般化的应用广泛的模型，其特点就是全局同步栅栏；    
2) TLAV是顶点中心化的编程模型，可同步（Pregel等），可异步；   
    
![]({{ site.baseurl }}/images/TLAV-0003.jpg)    

# [TLAV--去中心化的设计模式](TLAV--去中心化的设计模式)

1. 共享内存的图算法是中心化的；例如，Dijkstra最短路径算法，需要在内存里对任一顶点的随机访问；    
2. 数据太多，需要的内存远超一台计算机内存时（数千亿的顶点，数tb的内存），就需要分布式的，局部的，去中心化的方案；     
3. TLAV框架会迭代式地执行用户定义在顶点上的程序，从入边接收数据，向出边发送结果，随迭代的过程收敛计算结果；     
4. 显然TLAV框架的表达能力要弱于理想的图算法，但是其可以方便的应用到不同规模的图处理上；    
## [样例：单源最短路径算法TLAV实现](样例-单源最短路径算法TLAV实现)     

![]({{ site.baseurl }}/images/TLAV-0004.jpg)    

![]({{ site.baseurl }}/images/TLAV-0005.jpg)    

## [TLAV框架的四大支柱](TLAV框架的四大支柱)

1. Timing: 用户定义在顶点上的程序如何被调度执行；   
2. Communication: 顶点的程序数据如何被其它顶点程序获取；    
3. Execution Model: 顶点程序的执行和数据流的实现；    
4. Partitioning: 图的顶点如何被分别存储在由多个工作机组成的系统上；    
   
# [TLAV四大支柱](TLAV四大支柱)     

## [Timming](Timming)

1. 其描述了所有用于计算的激活的顶点如何被调度；    
2. 可以是同步，异步，或混合；     
   
![]({{ site.baseurl }}/images/TLAV-0006.jpg)    

### [Synchronous](Synchronous)

1. 基于BSP处理模型，所有激活的顶点在一个超级步的过程中并行执行；    
2. 有全局同步栅栏，在一个超级步间阻塞顶点程序进入下一个超级步，直到当前超级步的所有任务完成；    
3. 概念简单，可扩展，对特定类型的算法表现好，确定性强，方便应用的设计、编程、测试、排错、部署；    
4. 对于大图，可以很大程度平摊计算，运行时间根据顶点数量线性增长；    
5. 通常使用消息传递通信（message-passing communication），其可以应用高效地”batch messaging“方法，降低”计算通信比“；    
6. 缺点：   
1）同步时间长：高度划分的图上进行最短路径寻找，其同步时间占到了全部时间的80%；    
2）任务不平衡：激活顶点减少时，计算资源与计算任务之间变得不平衡，对于迭代式算法有”the curse of the last reducer“, 也就是少量任务可能需要很长时间用于收敛，”straggler problem“；    
3）可能不收敛：对于图着色算法，相邻两个顶点选择不同的颜色，在同步执行过程中，可能会出现两个相邻的顶点不断地在彼此的颜色之间切换的情况；    

### [Asynchronous](Asynchronous)  

1. 异步迭代模型，任一激活的顶点在可获得计算资源以及网络资源时就可以执行；     
2. 顶点执行顺序可以被调度动态生成以及重新调整，"straggler"问题消除，一般情况下异步模型优于同步模型，但是工程复杂度高；     
3. 异步模型不能受益于”batch messaging optimizations“，批量消息优化；       
4. 同步模式适合I/O-bound算法，异步模式适合CPU-bound算法；     
5. 许多迭代式算法有非对称收敛的特性，如PageRank算法，主要顶点一个超级步就能收敛，仅3%的顶点需要超10个超级步；异步执行时，可以动态调度，尽早处理高计算任务，以取得更好的性能；      
6. 异步更加灵活，可以动态根据资源进行调度；      
   
### [Hybrid](Hybrid)

1. 同步与异步各有优缺点，单一模式在面对不同场景时都很难取得最优的性能；     
2. PowerSwitch模型：     
1）将程序分为多个阶段，每隔一段时间评估下一个阶段更换模式的好处，预测未来模式的速度，每个阶段内采用同步或异步模式调度更新；     
2）运行前，性能采样器在很小的输入下运行算法几秒钟，然后再PowerSwitch上收集一些算法的执行统计信息，并存入知识库中；     
3）模式转换需要保证图数据的一致性；     
![]({{ site.baseurl }}/images/TLAV-0007.jpg)     
![]({{ site.baseurl }}/images/TLAV-0008.jpg)    

## [Communication](Communication)

1. TLAV框架下各个顶点上的程序如何共享数据；    
![]({{ site.baseurl }}/images/TLAV-0009.jpg)   

### [Message Passing](Message-Passing)   

1. 数据被包装成消息（网络消息）从一个顶点程序发送到另一个顶点程序；   
2. 当消息接收方为本机，可直接将消息放到目标顶点的接收队列；当消息接收方为远程机器，可将消息放到发送消息的消息缓冲池里，等待打包发送；     

```
“batch messaging optimizations” 即 批量消息优化，是在通信系统、分布式架构、数据传输等场景中，通过 “批量处理消息” 替代 “单条处理消息”，以解决高频次、小粒度消息传输带来的效率低、资源浪费等问题的一类优化技术。其核心目标是 降低通信开销、提升系统吞吐量、减少资源占用，广泛应用于消息队列（如 Kafka、RabbitMQ）、分布式数据库同步、移动应用推送、物联网（IoT）设备通信等领域。
一、核心背景：为什么需要批量消息优化？
在未优化的 “单条消息处理” 模式下，高频次小消息会带来两大关键问题，这也是批量优化的出发点：

通信开销过高：每条消息都需独立封装协议头（如 TCP 头部、应用层协议头）、建立 / 维护连接、进行网络握手，导致 “有效数据占比低”（例如 10 字节的消息可能携带 40 字节的协议头，有效载荷仅 20%）。
系统资源浪费：CPU 需频繁处理消息的接收、解析、转发逻辑，内存需频繁分配 / 释放小消息缓冲区，I/O 设备需频繁触发读写操作 —— 这些 “高频小额” 操作会显著增加系统上下文切换成本，降低整体吞吐量。
二、常见的批量消息优化技术
不同场景下的优化手段有所差异，但核心逻辑都是 “将多条消息聚合为一个批次后统一处理”，具体可分为以下几类：
1. 消息聚合（Message Aggregation）：核心优化手段
这是最基础的优化方式，通过 “延迟发送 / 累积消息” 将多条小消息打包成一个大批次，减少传输次数。关键策略包括：

    时间触发：设置 “批量窗口期”，若窗口期内累积的消息未达阈值，到点后仍强制发送批次（避免消息延迟过高）。
例：Kafka 生产者的 linger.ms 参数（默认 0，即实时发送；设置为 50 时，会等待 50ms 再发送，期间累积的消息会被批量打包）。
    数量触发：设置 “批量大小阈值”，当累积的消息数量 / 字节数达到阈值时，立即发送批次（避免批次过大导致传输延迟）。
例：RabbitMQ 的 batch-publish-size 参数，可设置每批次最多包含 100 条消息，达到后自动发送。
    混合触发：结合 “时间 + 数量” 双阈值，取先满足的条件触发批次发送（平衡 “吞吐量” 与 “延迟”）。
例：物联网设备向云端上报数据时，设置 “最多累积 50 条数据” 或 “最多等待 100ms”，哪个先到就触发批量上传。
1. 协议层优化：减少批次内冗余开销
即使消息已聚合，批次本身的协议头仍可能占用资源，需通过协议设计进一步优化：

    共享协议头：单个批次仅携带 1 个公共协议头，批次内的多条消息不再单独封装头部（仅保留消息体或简短标识）。
例：HTTP/2 的 “帧复用” 机制，将多个请求 / 响应封装为 “数据帧”，共享一个 TCP 连接和 HTTP 头部，本质是批量处理应用层消息。
    压缩批次数据：对整个消息批次进行压缩（如 Gzip、Snappy），而非单条消息压缩 —— 批量数据的重复度更高，压缩率通常比单条压缩提升 30%~50%，进一步减少网络传输量。
例：Kafka 支持对批次消息启用 Snappy 压缩，尤其适合日志类重复度高的消息场景。
1. 处理链路优化：减少批次的 “端到端延迟”
批量处理可能导致 “消息延迟增加”（需等待累积消息），需通过链路优化平衡 “批量效率” 与 “延迟”：

    异步批量处理：发送端 / 接收端采用 “异步线程池” 处理批次，避免批量逻辑阻塞业务线程。
例：分布式数据库的 “binlog 同步” 中，从库通过异步线程批量拉取主库的 binlog 批次，而非同步等待单条 binlog，既提升同步效率，又不阻塞从库的查询业务。
    动态批次调整：根据系统负载（如 CPU 使用率、网络带宽）自适应调整批次大小 / 窗口期 —— 负载高时增大批次（减少资源占用），负载低时减小批次（降低延迟）。
例：微服务框架 Dubbo 的 “批量调用” 功能，可根据服务响应时间动态调整每批次的调用次数，避免高峰期因批次过大导致超时。
1. 存储层优化：减少批次的持久化开销
若消息需持久化（如消息队列的 “消息落盘”），批量处理可显著减少 I/O 操作：

    批量写入磁盘：将批次消息一次性写入磁盘的连续块（而非单条消息分散写入），减少磁盘寻道时间（机械硬盘的寻道时间通常占 I/O 耗时的 70% 以上）。
例：Kafka 的 “日志分段（Log Segment）” 机制，将消息批量写入大小固定的分段文件，而非单条追加，极大提升了磁盘写入吞吐量。
    内存缓冲批次：在内存中维护 “批次缓冲区”，仅当批次满或到时间时才触发磁盘写入，减少 “随机写” 为 “顺序写”（顺序写的效率是随机写的 10~100 倍）。
三、优化效果与权衡：批量的 “利” 与 “弊”
批量消息优化的核心是 “用可控的延迟换取更高的吞吐量和资源效率”，需在实际场景中平衡以下权衡：

优化收益（利）潜在代价（弊）典型解决策略
降低网络带宽占用（减少协议头和传输次数）消息延迟增加（需等待批次累积）用 “混合触发”（时间 + 数量阈值）控制延迟上限，敏感场景设置小窗口期
减少 CPU / 内存开销（减少上下文切换和缓冲区操作）批次失败风险扩大（单批次丢失会影响多条消息）引入 “批次确认机制”（如 Kafka 的 ACK 机制），失败后重试整个批次
提升磁盘 I/O 效率（将随机写转为顺序写）内存占用临时升高（需缓存批次消息）限制最大批次大小，避免内存溢出；采用 “内存池” 复用缓冲区
四、典型应用场景举例
消息队列（Kafka/RabbitMQ）：生产者通过 “linger.ms”“batch.size” 参数批量发送消息，消费者通过 “批量拉取”（如 Kafka 的 fetch.min.bytes）减少拉取次数，提升整体吞吐量。
移动应用推送（APNs/FCM）：推送服务商将同一设备 / 同一用户的多条推送消息批量打包，通过 1 次网络请求发送，减少移动端的网络唤醒次数（节省电量）。
物联网（IoT）：传感器（如温湿度传感器）每 10 秒采集 1 条数据，但设置 “每 5 分钟批量上报 30 条数据”，避免高频小数据导致的网络拥堵。
分布式数据库同步：主库将多条 SQL 执行记录（binlog）批量发送给从库，从库批量执行 binlog，减少主从同步的延迟和网络开销。

综上，“批量消息优化” 不是单一技术，而是一套围绕 “聚合消息、减少冗余、平衡效率与延迟” 的技术体系，其核心价值是解决 “高频小消息” 场景下的系统性能瓶颈，是分布式系统、通信架构中提升吞吐量的关键手段。
```

3. 消息传递一般在同步模式下应用，同步模式显著受益于批量消息优化；例如，PageRank算法的所有节点都是激活态，需要频繁的进行消息传递，此算法在同步模式下效率显著优于异步模式；     

### [Shared Memory](Shared-Memory)

1. 顶点数据被直接共享，可被其它顶点读写；     
2. 分布式共享内存，打破物理内存边界，隐藏底层网络通信细节和数据的一致性，地址映射→数据定位→远程传输→一致性维护，软硬件结合等待；      
3. TLAV框架的单机模式下，可以方便使用共享内存；困难的是在分布式系统下的共享内存，机器之间实际上还是需要网络消息进行通信；      
4. Trinity框架：在分布式内存上抽象了全局共享内存地址，用64位的hash码进行内存块和机器之间的映射；      
5. 需要用互斥操作保证内存的一致性，让并行执行的程序进入一致性执行的序列，以此保证内存一致，例如哲学家就餐问题；      
6. 用一个备份边界顶点，”幽灵“，去跟其它相邻点交互，此时只需要保证幽灵顶点与原始顶点之间的一致性，一般使用一个分布式的锁；       
7. 一般性结论：异步的共享内存系统性能上要优于同步的消息传递系统；    
    
### [Active Messages](Active-Messages)

1. 不管是消息传递还是分布式共享内存，其都在分布式操作系统上应用；”主动消息“机制应用于GRE框架下（Generic Routing Encapsulation，通用路由封装）；      
    
```
GRE（Generic Routing Encapsulation，通用路由封装）是一种三层隧道协议，核心作用是将一种网络层协议的数据包（如 IPv4、IPv6）封装在另一种网络层协议的数据包中，实现不同网络环境下的数据跨域传输，尤其常用于构建 VPN（虚拟专用网络）或解决网络地址冲突问题。
一、GRE 的核心原理：“数据包套娃”
GRE 的本质是双层封装，通过在原始数据包外层添加新的 “头部信息”，让数据包能在 “中间网络” 中被正确路由，到达目标后再 “解包” 恢复原始数据。整个过程类似给包裹套上快递盒，具体步骤如下：

    封装：发送端将原始数据包（如内网 IPv4 数据包，称为 “载荷数据包”）作为数据部分，外层添加GRE 头部和外层网络头部（通常是公网 IPv4 头部）。
    传输：封装后的数据包通过中间网络（如互联网）传输，中间路由器仅根据 “外层头部” 的目标 IP 地址进行路由，无需关心内部的原始数据包内容。
    解封装：接收端（或 GRE 隧道终点）收到数据包后，先剥离外层网络头部和 GRE 头部，恢复出原始的载荷数据包，再转发到目标内网。
二、GRE 的关键特点
1. 协议无关性（“通用” 的核心体现）
GRE 最显著的特点是不绑定特定内层或外层协议，灵活性极高：

内层（载荷）协议：支持 IPv4、IPv6、IPX、AppleTalk 等多种网络层协议，甚至可以封装 GRE 数据包（即 “GRE 嵌套”）。
外层协议：通常使用 IPv4，但也支持 IPv6。
这意味着它能连接不同协议的网络，比如将 IPv6 数据包封装在 IPv4 隧道中，解决当前 IPv4 与 IPv6 网络共存期的互通问题。
2. 轻量级设计，开销低
GRE 头部结构简单，仅包含4 个固定字节（标记、版本等）+ 可选字段（如密钥、序列号），相比 IPsec、L2TP 等协议，额外的 “封装开销” 极小：

若不启用可选字段，单个 GRE 数据包仅增加 4 字节头部；即使启用序列号（用于防数据包乱序），也仅增加 8 字节。
这种轻量级特性使其适合对带宽开销敏感的场景，如中小规模企业 VPN。
3. 支持多播与广播（部分场景）
与仅支持单播的 IPsec（传输模式）不同，GRE 隧道可以透传多播和广播数据包：

例如，内网中的组播视频流（如 IPTV）、动态路由协议（如 OSPF、EIGRP）的组播更新包，可通过 GRE 隧道在异地内网间传输，无需额外配置。
注意：部分公网环境可能限制多播流量，此时需结合其他技术（如 PIM 协议）确保多播数据包能在隧道中正常转发。
4. 无加密与认证（需额外补充）
GRE 是 “纯封装协议”，本身不提供数据加密、身份认证和完整性校验：

封装后的数据包在公网传输时，原始数据内容是明文的，存在被窃听、篡改的风险。
因此，实际应用中 GRE 通常与 IPsec 结合（即 “GRE over IPsec”）：GRE 负责封装和透传多播 / 广播，IPsec 负责加密和认证，兼顾灵活性与安全性。
5. 可解决地址冲突与私网互联
当两个异地内网使用相同的私网 IP 段（如均为 192.168.1.0/24）时，直接通信会因地址冲突失败，而 GRE 可通过 “隧道隔离” 解决：

两地网关通过公网 IP 建立 GRE 隧道，原始私网数据包被封装在公网头部中传输，中间网络仅识别公网 IP，不处理私网地址，从而避免冲突。
三、GRE 的典型应用场景
企业 Site-to-Site VPN
连接异地分公司内网，让员工访问跨地域的共享文件、数据库等资源，就像在同一局域网内操作。通常与 IPsec 结合，确保数据安全。
IPv4 与 IPv6 互通
在 IPv6 尚未全面普及的阶段，通过 “IPv6 over GRE over IPv4” 或 “IPv4 over GRE over IPv6” 的隧道模式，实现两种协议网络的通信。
动态路由协议透传
让 OSPF、BGP 等动态路由协议的数据包通过 GRE 隧道传输，实现异地内网路由表的自动同步，减少手动配置路由的工作量。
云资源访问（混合云场景）
企业本地数据中心与公有云（如 AWS、阿里云）之间建立 GRE 隧道，安全访问云中的虚拟机、存储等资源，避免数据通过公网明文传输.
```

2. 主动消息的核心特点是消息包含数据与计算操作（data and operator），避免先接收再调度的额外开销；     
3. 在GRE架构下，主动消息将消息发送和接收的过程结合，移除中间状态的存储，如消息队列等；    
4. GRE框架原始图数据修改为Agent-Graph，此数据结构仅被框架自己使用，用户不感知；”combiner“ 和 ”scatter“被用来减少多机之间的通信；    

### [Message Passing Optimizations](Message-Passing-Optimizations)

![]({{ site.baseurl }}/images/TLAV-0010.jpg)     

1. 根据多机之间的连接关系进行优化，根据多机的性能进行优化；    

## [Execution Model](Execution-Model)

### [Vertex Program Implementation](Vertex-Program-Implementation)

1. 顶点程序被实现为三种“相模型”(phase-models)，或“边中心”。模式的选择不影响最终结果；    
1）One Phase: 顶点程序被抽象化为一个函数，根据入边的消息，逐次执行数据更新操作，逐次向出边发送新的计算结果；     
2）Two Phase：顶点程序被分为两个函数，一般采用Scatter-Gather模型，scatter负责分发当前的数据，gather负责收集输入然后进行数据更新；    
3）Three Phase：Gather-Apply-Scatter模型，Gather对入边执行“加和”，Apply执行顶点数据更新，Scatter负责分发；     
4）Edge-Centric: 模型以边为算法迭代执行的单元；     

### [Push vs. Pull](Push-vs.-Pull)

1. 顶点间的信息流动可以描述为两个动作“push” 和“pull”；从激活的顶点“push”到邻居顶点，从邻居顶点“pull”到激活的顶点；     
2. 追踪数据的变化，来减少不必要的拉取操作；    
    
## [Partitioning](Partitioning)

![]({{ site.baseurl }}/images/TLAV-0011.jpg)      

1. 大型图数据结构必须划分成多个部分，使得分布式的内存能够存得下；     
2. 好的划分可以显著提高性能，例如，划分使得计算资源负载均衡，最小化划分边的数量以减少网络阻塞，这就是“k-way graph partition”问题，其是NP完全问题，其本身就是重要的研究方向；     

![]({{ site.baseurl }}/images/TLAV-0012.jpg)     


3. METIS（Multi- level graph partitioning schemes.）三个阶段，多级划分，已经十分接近最优划分了，主要缺点是耗时过长和内存需求大，需要对所有节点随机访问；      

### [Distributed Heuristics](Distributed-Heuristics)     

1. 分布式的启发式算法，划分没有重叠，需要指定划分数量，受启发于分布式网络社区检测，如“label propagation”；    
2. 标签传播算法需要平衡划分，避免单一标签出现比重过高；顶点级别的迭代，使得算法能够分布式并行；    
   
### [Streaming](Streaming)

1. 流式划分，过一遍图来实现图的划分，对于TLAV框架来说，流式划分可在图从硬盘载入集群的过程中运行；     
2. LDG（linear deterministic greedy），把顶点分配到与其共享边数量最多的划分，同时根据划分的剩余容量进行惩罚；    
   
### [Verte Cuts](Verte-Cuts)

1. 用剖分顶点的方式替代剖分边；边剖分把边分配到机器，顶点剖分导致顶点被多机共享；      
2. 边划分一般由于顶点划分；     
   
### [Dynamic Repartitioning](Dynamic-Repartitioning)

1. TLAV框架下，每次迭代激活顶点的分布都不一致，动态再划分用来保持负载均衡，在必要的情况下合并多个节点上的顶点；      
   
# [TLAV的实现](TLAV的实现)      

## [System Architecture](System-Architecture)

1. TLAV框架一般采用主从模式（“master-slave”）并行架构（一般可以把并行分为，主从模式，对等模式，或混合）；     
   
## [Multi-Core Support](Multi-Core-Support)

1. 在多核计算节点上，一般是一个划分分配一个核，也可以使用多线程充分利用计算资源；   

## [Fault Tolerance](Fault-Tolerance)

1. 分布式系统经常遇到部分计算节点失败的情况，当节点失败后，其所有数据就会丢掉，所有要设立“checkpoint”，用来容错；     
   
## [Single Machine Architectures](Single-Machine-Architectures)

1. 单机框架方便调试程序，但是没有足够的内存处理大规模的图；     
2. 压缩：Twitter上的服务，在单机上处理图，单机有144GB RAM，一条边需要5个字节；    


